{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSF 519.01 Applied Data Science \n",
    "**Assignment 4** - 100 marks\n",
    "\n",
    "**Due:** November 15, 04.00 pm.\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE: each task must be implemented as asked, even if there are other easier or better solutions.**\n",
    "\n",
    "**How to deliver:**\n",
    "Edit this file and write your solutions in sections specified with `# Your solution`. Test your code and when you are done, submit this notebook as an `.ipynb` file to D2L dropbox. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Clustering and Classification (45 points)\n",
    "\n",
    "### Part A. Clustering\n",
    "In this problem, we apply two clustering algorithms on MNIST handwritten digits data and compare them using ARI score. Here you can see how image data are usually represented for machine learning tasks. Complete the code below to load the MNIST data, feel free to add more code (plotting graphs or printing values) to explore the dataset. In the end, since many algorithms work better or can work only with normalized data, use `scale` method in scikit learn to normalize the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_data, number of features) = (1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  5., 12., 13., 16., 16.,  2.,  0.,  0., 11., 16., 15.,  8.,\n",
       "        4.,  0.,  0.,  0.,  8., 14., 11.,  1.,  0.,  0.,  0.,  0.,  8.,\n",
       "       16., 16., 14.,  0.,  0.,  0.,  0.,  1.,  6.,  6., 16.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  5., 16.,  3.,  0.,  0.,  0.,  1.,  5., 15.,\n",
       "       13.,  0.,  0.,  0.,  0.,  4., 15., 16.,  2.,  0.,  0.,  0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  5.17802955,  1.42951714,  0.27407152,  0.96867267,\n",
       "        1.80378259,  0.19180703, -0.12502292, -0.05907756,  2.81857826,\n",
       "        1.03648545,  0.75962245, -0.4767178 , -0.69007742, -0.51499146,\n",
       "       -0.13043338, -0.04462507,  1.5099244 ,  0.72010829,  0.69077777,\n",
       "       -0.98767917, -1.25998248, -0.54880546, -0.11422184, -0.03337973,\n",
       "        1.7580838 ,  1.11605583,  1.22058589,  0.66221904, -1.28625035,\n",
       "       -0.62889588, -0.04723238,  0.        , -0.38496671, -0.263679  ,\n",
       "       -0.49018022,  0.96064411, -1.48986148, -0.82269451,  0.        ,\n",
       "       -0.06134367, -0.5312841 , -1.05283456, -0.34600957,  1.33078862,\n",
       "       -0.91966262, -0.79827225, -0.08874162, -0.03543326,  0.16927197,\n",
       "       -0.44426532,  1.04502442,  0.67610628, -1.45261152, -0.75743581,\n",
       "       -0.20978513, -0.02359646,  3.98338224,  1.85087135,  0.89424571,\n",
       "       -1.98864237, -1.14664746, -0.5056698 , -0.19600752])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "digits = load_digits()\n",
    "data = digits['data']  # Your solution\n",
    "labels = digits['target']\n",
    "\n",
    "print('(n_data, number of features) =', data.shape)  # = (1797, 64)\n",
    "# There are 64 features per sample, since each sample is a 8*8 image.\n",
    "\n",
    "# Let's see a sample:\n",
    "display(data[15], labels[15])\n",
    "plt.imshow(data[15].reshape((8, 8)));  # See that nice little 5 there?\n",
    "\n",
    "# Scale and normalize the feature values using:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html\n",
    "\n",
    "data = scale(data)\n",
    "display(data[15], labels[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the pixel values as the features, apply k-means clustering to cluster the digits together. Try K-means with number of clusters = 10, use ARI score to compare KMeans clustereing results with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means ARI score = 0.57005\n"
     ]
    }
   ],
   "source": [
    "# Your solution\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(data)\n",
    "prediction = kmeans.labels_\n",
    "ARI_score = metrics.adjusted_rand_score(prediction, labels)\n",
    "\n",
    "print(f'K-Means ARI score = {ARI_score:.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply agglomerative clustering technique on the data. Again set cut-off number of clusters to 10. Try 4 different values of linkage (ward, complete, average, and single) and 3 distance calculation modes (affinity): euclidean, L1, and L2. Report ARI score wrt ground truth for each method. Since ward linkage can only work with euclidean distance mode, skip L1 and L2 when using ward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHC Affinity=euclidean Linkage mode=ward     ARI score = 0.66435\n",
      "AHC Affinity=euclidean Linkage mode=complete ARI score = 0.00029815\n",
      "AHC Affinity=l1        Linkage mode=complete ARI score = 0.1921\n",
      "AHC Affinity=l2        Linkage mode=complete ARI score = 0.00029815\n",
      "AHC Affinity=euclidean Linkage mode=average  ARI score = 2.345e-05\n",
      "AHC Affinity=l1        Linkage mode=average  ARI score = 0.00053779\n",
      "AHC Affinity=l2        Linkage mode=average  ARI score = 2.345e-05\n",
      "AHC Affinity=euclidean Linkage mode=single   ARI score = 9.454e-06\n",
      "AHC Affinity=l1        Linkage mode=single   ARI score = 6.6739e-06\n",
      "AHC Affinity=l2        Linkage mode=single   ARI score = 9.454e-06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "for linkage_mode in ('ward', 'complete', 'average', 'single'):\n",
    "    for affinity in ('euclidean', 'l1', 'l2'):\n",
    "        # Your solution\n",
    "        if(linkage_mode=='ward' and (affinity=='l1' or affinity=='l2')):\n",
    "            continue\n",
    "        \n",
    "        prediction = AgglomerativeClustering(n_clusters=10, linkage=linkage_mode, affinity=affinity).fit_predict(data)\n",
    "        ARI_score = metrics.adjusted_rand_score(prediction, labels)\n",
    "        \n",
    "        print(f'AHC Affinity={affinity:9} Linkage mode={linkage_mode:8} ARI score = {ARI_score:.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B. Multiclass Classification\n",
    "\n",
    "Now we are going to use the same dataset but for a different task, which is multi-class classification. The 64 pixel values will be the features and the labels are the actual written digit. Train a Linear SVC classifier and compare it with an ensemble of LogisticRegression classifiers. SVC and Logistic Regression are binary classifiers at their cores, but their scikit-learn implementation automatically handles the one-versus-rest scheme for you. For logistic regression try both 'ovr' (one-versus-rest) and 'multinomial' for multiclass handling method. Keep the scores for all four classification methods and draw a box plot in the end to compare them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAVHElEQVR4nO3df5BlZX3n8feHAWbCD5GBKXQZYNgEXScGEVvURDOE1RRU7YIwbgRdE7Y2sCRh3ICk+JHdiLMFpBLUGMWMqCi4CUg0a2GMC9mRH9nNktCEXwI1ZEAThknpsBAsIAQGv/vHPcNcmwe6Z6ZP3+6e96vqFuee89x7v91nuJ9+nnPOc1JVSJI00S6jLkCSNDsZEJKkJgNCktRkQEiSmgwISVLTrqMuYLrsv//+tWzZslGXIUlzyu233/5oVS1pbZs3AbFs2TLGx8dHXYYkzSlJ/u6ltjnEJElqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVLTvLlQTpK2R5JpeZ/5eG8dA0LSTm2yL/Yk8/LLfyocYpIkNRkQkqQmA0KS1GRASJKaeg2IJMcmWZdkfZLzGtsPSbI2yd1JbkqydGjb7yS5N8n9SX4/03WqgSRpSnoLiCQLgMuA44DlwClJlk9odilwVVUdDqwGLule+9PAzwCHA68H3gys6KtWSdKL9dmDOApYX1UPVdWzwDXACRPaLAfWdss3Dm0vYBGwO7AQ2A34Xo+1SpIm6DMgDgQeHnq+oVs37C5gZbd8IrB3kv2q6v8yCIx/6B7XV9X9PdYqSZqgz4BoHTOYeLXJOcCKJHcwGEJ6BNic5CeA1wFLGYTKMUl+9kUfkJyeZDzJ+KZNm6a3eknayfUZEBuAg4aeLwU2Djeoqo1VdVJVvRH4zW7dEwx6E7dW1ZNV9STwTeCtEz+gqi6vqrGqGluypHnPbUnSduozIG4DDktyaJLdgZOB64YbJNk/yZYazgeu6Jb/nkHPYtckuzHoXTjEJEkzqLeAqKrNwJnA9Qy+3K+tqnuTrE5yfNfsaGBdkgeAA4CLuvVfAR4E7mFwnOKuqvp6X7VKkl4s82USqrGxsRofHx91GZLmmfk+WV+S26tqrLXNK6klSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUtOuoy5AmuuSTMv7VNW0vI80XQwIaQdN5Ys9iQGgOcchJknz2uLFi0my3Q9gh16fhMWLF4/4t7B97EFImtcef/zxkffepmsYcqbZg5AkNRkQkqQmA0KS1GRASJKaDAhJUpNnMc0SXmwlabYxIGaJyb7YvdBK0kzrdYgpybFJ1iVZn+S8xvZDkqxNcneSm5Is7db/XJI7hx7PJHl3n7VKkn5UbwGRZAFwGXAcsBw4JcnyCc0uBa6qqsOB1cAlAFV1Y1UdUVVHAMcATwM39FWrJOnF+uxBHAWsr6qHqupZ4BrghAltlgNru+UbG9sB3gN8s6qe7q1SSdKL9BkQBwIPDz3f0K0bdhewsls+Edg7yX4T2pwMXN36gCSnJxlPMr5p06ZpKFmStEWfAdE6LWfiUdZzgBVJ7gBWAI8Am194g+TVwE8B17c+oKour6qxqhpbsmTJ9FQtSQL6PYtpA3DQ0POlwMbhBlW1ETgJIMlewMqqemKoyS8A/6OqnuuxTklSQ589iNuAw5IcmmR3BkNF1w03SLJ/ki01nA9cMeE9TuElhpckSf3qLSCqajNwJoPhofuBa6vq3iSrkxzfNTsaWJfkAeAA4KItr0+yjEEP5Oa+apQkvbTMl4uvxsbGanx8fNRl9MYL5eY2998IXbjPqCsYuPCJyduMQJLbq2qstc0rqSXNa/nID0YezkmoC0dawnZxsj5JUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiTWLx4MUl26AHs8HssXrx4xL8J7Wy8UE6axOOPPz7yC61g+u5bLk2VPQhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmrxQTtK8N+qLDPfdd9+Rfv72MiAkzWs7ehX8znw/cYeYJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQbEDNnRG9+DN72XNLOmPNVGkrcDh1XVF5IsAfaqqu/0V9r8MhtufD/q+WgkzS1T6kEk+TBwLnB+t2o34L/3VZQkafSmOsR0InA88BRAVW0E9u6rKEnS6E01IJ6twfhIASTZcyovSnJsknVJ1ic5r7H9kCRrk9yd5KYkS4e2HZzkhiT3J7kvybIp1ipJmgZTDYhrk3wGeGWS04D/BXz25V6QZAFwGXAcsBw4JcnyCc0uBa6qqsOB1cAlQ9uuAn63ql4HHAV8f4q1SpKmwZQOUlfVpUneBfwAeC3wW1X155O87ChgfVU9BJDkGuAE4L6hNsuBs7rlG4GvdW2XA7tu+YyqenJqP44kabpMGhBdT+D6qnonMFkoDDsQeHjo+QbgLRPa3AWsBD7B4DjH3kn2A14D/GOSPwEOZdBjOa+qnp9Q2+nA6QAHH3zwNpQmTV19+BVw4T6jLmNQhzSDJg2Iqno+ydNJ9qmqJ7bhvVvnVE48z/Mc4FNJTgVuAR4BNnd1vQN4I/D3wJeBU4HPT6jtcuBygLGxsZ3zlk/qXT7yg5Gfogzdnc0uHHUV2plM9TqIZ4B7kvw53ZlMAFX1wZd5zQbgoKHnS4GNww26s6FOAkiyF7Cyqp5IsgG4Y2h46mvAW5kQEJKk/kw1IL7RPbbFbcBhSQ5l0DM4GXjfcIMk+wOPVdUPGVxjccXQa/dNsqSqNgHHAOPb+PmSpB0w1YPUVybZncGxAYB1VfXcJK/ZnORM4HpgAXBFVd2bZDUwXlXXAUcDlyQpBkNMv9a99vkk5wBrM7j893YmOWtKkjS9MpWx1SRHA1cC32VwbOEg4Jeq6pY+i9sWY2NjNT4+ezsZSUY+jj0bapiLZsvvbbbUsbOZ77/3JLdX1Vhr21SHmD4K/HxVreve8DXA1cCbpqdESdJsM9UL5XbbEg4AVfUAg/mYJEnz1FR7EONJPg98qXv+fgbHBSRJ89RUA+JXGBxA/iCDYxC3AJ/uqyhJ0uhNNSB2BT5RVR+DF66uXthbVZKkkZvqMYi1wI8NPf8xBtNfSJLmqakGxKLhCfO65T36KUmSNBtMNSCeSnLklidJxoB/6qckSdJsMNVjEL8O/HGSjQwm3PsXwHt7q0qSNHIv24NI8uYkr6qq24B/xWBW1c3A/wS+MwP1SZJGZLIhps8Az3bLbwMuYHCXuMfpptmWJM1Pkw0xLaiqx7rl9wKXV9VXga8mubPf0iRJozRZD2JBki0h8q+Bbw1tm+rxC0nSHDTZl/zVwM1JHmVw1tJfACT5CWBb7i4nSbPS4I4CO95mPs74+rIBUVUXJVkLvBq4obb+BnYBVvVdnCT1bT5+sU+XqdyT+tbGugf6KUeSNFtM9UI5SdJOxoCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSk3eFk6ZgKjeM6du+++476hK0kzEgpElMxw1lknhjGs05BsQMqQ+/Ai7cZ/Q1SNIUGRAzJB/5wcj/gkxCXTjSEiTNIb0epE5ybJJ1SdYnOa+x/ZAka5PcneSmJEuHtj2f5M7ucV2fdUqSXqy3HkSSBcBlwLuADcBtSa6rqvuGml0KXFVVVyY5BrgE+EC37Z+q6oi+6pMkvbw+exBHAeur6qGqeha4BjhhQpvlwNpu+cbGdknSiPQZEAcCDw8939CtG3YXsLJbPhHYO8l+3fNFScaT3Jrk3a0PSHJ612Z806ZN01m7JO30+gyI1onjE4/SngOsSHIHsAJ4BNjcbTu4qsaA9wG/l+THX/RmVZdX1VhVjS1ZsmQaS5ck9XkW0wbgoKHnS4GNww2qaiNwEkCSvYCVVfXE0Daq6qEkNwFvBB7ssV5J0pA+exC3AYclOTTJ7sDJwI+cjZRk/yRbajgfuKJbv2+ShVvaAD8DDB/cliT1rLeAqKrNwJnA9cD9wLVVdW+S1UmO75odDaxL8gBwAHBRt/51wHiSuxgcvP7tCWc/SZJ6llFfvDVdxsbGanx8fNRlvKTZMNXCbKhhZ+XvXrNVktu7470v4myukqQmA0KS1GRASJKaDAhJUpMBIUlqMiBmUJKRPrwjmTR1q1atYtGiRSRh0aJFrFq1atQlzTgDYoZU1Q49puM9HnvssRH/FqS5YdWqVaxZs4aLL76Yp556iosvvpg1a9bsdCHhdRBzhOfRz23uv7ll0aJFXHzxxZx99tkvrPvYxz7GBRdcwDPPPDPCyqbfy10HYUDMEX7BzG3uv7klCU899RR77LHHC+uefvpp9txzz3m3H71QTpK2wcKFC1mzZs2PrFuzZg0LFy4cUUWj4T2pJWmC0047jXPPPReAM844gzVr1nDuuedyxhlnjLiymWVASNIEn/zkJwG44IIL+NCHPsTChQs544wzXli/s/AYxBzhGPbc5v7TbOUxCEnSNjMgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1OT9IKQdlGRa2jkduGYbA0LaQX6xa75yiEmS1GRASJKaDAhJUpMBIUlq6jUgkhybZF2S9UnOa2w/JMnaJHcnuSnJ0gnbX5HkkSSf6rNOSdKL9RYQSRYAlwHHAcuBU5Isn9DsUuCqqjocWA1cMmH7fwNu7qtGSdJL67MHcRSwvqoeqqpngWuAEya0WQ6s7ZZvHN6e5E3AAcANPdYoSXoJfQbEgcDDQ883dOuG3QWs7JZPBPZOsl+SXYCPAr/xch+Q5PQk40nGN23aNE1lS5Kg34BoXTY68Yqic4AVSe4AVgCPAJuBXwX+rKoe5mVU1eVVNVZVY0uWLJmOmiVJnT6vpN4AHDT0fCmwcbhBVW0ETgJIshewsqqeSPI24B1JfhXYC9g9yZNV9aID3ZKkfvQZELcBhyU5lEHP4GTgfcMNkuwPPFZVPwTOB64AqKr3D7U5FRgzHCRpZvU2xFRVm4EzgeuB+4Frq+reJKuTHN81OxpYl+QBBgekL+qrHknStsl8mWhsbGysxsfHR11Gb5I4KZykaZfk9qoaa23zSmpJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ19Xk/CG2DpHUDvm1v44yvkqaLATFL+MUuabZxiEmS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkpsyXC7SSbAL+btR19Gh/4NFRF6Ht5v6bu+b7vjukqpa0NsybgJjvkoxX1dio69D2cf/NXTvzvnOISZLUZEBIkpoMiLnj8lEXoB3i/pu7dtp95zEISVKTPQhJUpMBIUlqMiBmiSS/meTeJHcnuTPJN5NcMqHNEUnu75b3SvKZJA92r7slyVtGU/3OIcmTjXUXJnmk22f3JTllFLVpcu6/bWdAzAJJ3gb8G+DIqjoceCfw28B7JzQ9GfijbvlzwGPAYVX1k8CpDC7o0cz7eFUdAZwAfCbJbtP55km882O/et1/c5kBMTu8Gni0qv4ZoKoeraqbgX+c0Cv4BeCaJD8OvAX4L1X1w+41D1XVN2a6cG1VVX8LPA3sO3FbkkOSrO16iGuTHJxknyTfTbJL12aPJA8n2S3JTUkuTnIz8J9n+EfZKU2y/76Y5A+S3JjkoSQrklyR5P4kXxxq92SSi5LcleTWJAfM4I8w7QyI2eEG4KAkDyT5dJIV3fqrGfQaSPJW4P91/4h/Erizqp4fTblqSXIk8LdV9f3G5k8BV3U9xD8Efr+qngDuArbs738LXF9Vz3XPX1lVK6rqo33Xrkn3HwyC4xjgLODrwMcZ/L/4U0mO6NrsCdxaVW8AbgFO67fqfhkQs0BVPQm8CTgd2AR8OcmpwDXAe7q/ME9mEBiafc5Ksg74K+DCl2jzNrYOD34JeHu3/GW2DiWe3D1naJv6N5X9B/D1GlwXcA/wvaq6p+vB3wss69o8C/xpt3z70Po5yYCYJarq+aq6qao+DJwJrKyqh4HvMvgLcyVwbdf8XuANW4YmNHIfr6rXMviivyrJoim8ZssFSNcBxyVZzOCPhG8NtXlqesvUS5jq/vvn7r8/HFre8nzLcaLnauvFZc8PrZ+T/IKZBZK8NslhQ6uOYOvMtFcz6Mo+WFUbAKrqQWAc+EiSdO9xWJITZrBsTVBVf8Jgv/xSY/Nf0g0XAu8H/nf3mieBvwY+Afypw4ajM8n+2ykZELPDXsCV3Wl2dwPL2drV/WMG45zXTHjNLwOvAtYnuQf4LLBxZsrdae2RZMPQ4+xGm9XA2Y3e3QeB/9Dt3w/woweevwz8exxS6tuO7L+dklNtSJKaTElJUpMBIUlqMiAkSU0GhCSpyYCQJDUZENKQJK9Kck03S+59Sf4syWuSfHsaP2N1knd2y+/oZuO9M8mBSb4yXZ8j7ShPc5U63UWHfwlcWVVrunVHAHsDf1BVr+/hM9cAf1VVX9iO1y7wwjr1yR6EtNXPMZgqYc2WFVV1J/DwludJliX5iyR/0z1+ulv/6u6eHHcm+XbXM1jQzQL67ST3JDmra/vFJO9J8ssMZuj9rSR/2L33t7s2C5L8bpLbuhlg/1O3/uhuRtE/YjAnkNSbOT1PiDTNXs9ggrWX833gXVX1TDc9ytXAGPA+BjOxXpRkAbAHgylTDtzS80jyyuE3qqrPJXk7gyk2vpJk2dDm/wg8UVVvTrIQ+D9Jbui2HQW8vqq+syM/rDQZA0LaNrsBn+qGnp4HXtOtvw24orvZzNeq6s4kDwH/MskngW8wmNZ9qn4eODzJe7rn+wCHMZgt9K8NB80Eh5ikre5lMKPqyzkL+B7wBgY9h90BquoW4GeBR4AvJfnFqnq8a3cT8GsM7gI4VQFWVdUR3ePQqtoSMM7yqhlhQEhbfQtYmOSFm7wkeTNwyFCbfYB/6O4D8AFgQdfuEOD7VfVZ4PPAkUn2B3apqq8C/xU4chtquR74la5HQncm1Z7b/6NJ284hJqlTVZXkROD3kpwHPMPgfhy/PtTs08BXk/w74Ea2/jV/NPAbSZ4DngR+ETgQ+MLQzKDnb0M5n2Nws5m/6c6u2gS8ezt+LGm7eZqrJKnJISZJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktT0/wE9v075VR4jVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "# Ignore increasing max iterations warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "scores = {name: [] for name in ('SVC', 'LR ovr', 'LR mn')}\n",
    "for seed in np.arange(1, 30+1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, random_state=seed)\n",
    "    \n",
    "    # Your solution\n",
    "    svc_score = LinearSVC().fit(X_train, y_train).score(X_test, y_test)\n",
    "    scores['SVC'].append(svc_score)\n",
    "    lr_ovr_score = LogisticRegression(solver='liblinear', multi_class='ovr').fit(X_train, y_train).score(X_test, y_test)\n",
    "    scores['LR ovr'].append(lr_ovr_score)\n",
    "    lr_mn_score = LogisticRegression(solver=\"lbfgs\", multi_class='multinomial').fit(X_train, y_train).score(X_test, y_test)\n",
    "    scores['LR mn'].append(lr_mn_score)\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.boxplot([scores['SVC'], scores['LR ovr'], scores['LR mn']], labels=['SVC', 'LR ovr', 'LR mn'])\n",
    "axis.set_xlabel(\"Classifier\")\n",
    "axis.set_ylabel(\"Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Dimension reduction for feature selection (25 points)\n",
    "\n",
    "In the first part of this problem, we compare the dimension reduction ability of PCA and linear regression models.\n",
    "\n",
    "To use linear regression model as a dimension reduction technique, we fit the model and pick the most informative features (highest absolute coefficients) and ignore all other ones. \n",
    "\n",
    "Our dataset is \"LLVMAll.csv\", which records some performance measurements of a LLVM Compiler. See https://zenodo.org/record/322483#.Xbmov0VKjOT for more details.\n",
    "\n",
    "- Read the dataset and apply proper normalization on each row. Note that you only apply it on the feature columns and leave the target values (performance) untouched.\n",
    "- Apply Linear Regression with default paramters to pick the two most important features (use `top_k_indices`).\n",
    "- Fit a PCA model (with default paramters) on the original training set to find the first two principal components.\n",
    "- Now for comparison, build 3 Linear Regression models only differing on their input/output data: \n",
    "    1. use the original training and test sets\n",
    "    2. use the train and test set given by linear regression selected features\n",
    "    3. use the PCA transformed data\n",
    "- Repeat the steps above 30 times with random_state range [1..30] for train/test splitter and calculate the test scores in the end. Note that for each run a new PCA and Linear regression should be applied (i.e., do not transform new data based on the old fitted models)\n",
    "- Report the median scores over 30 runs for each of the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0 "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def read_and_normalize(file_name):\n",
    "    # Your solution\n",
    "    data = pd.read_csv(file_name)\n",
    "    features = normalize(data.drop(columns='$<Performance'))\n",
    "    labels = data['$<Performance']\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def top_k_indices(l, k=1):\n",
    "    \"\"\"\n",
    "    Return indices of top k elements of l.\n",
    "    \n",
    "    Example:\n",
    "    for index in top_k_indices([5, 0, 0, 4, 10, 1], k=2):\n",
    "        print(index)\n",
    "        \n",
    "    result: 4 0\n",
    "    \"\"\"\n",
    "    # Your solution\n",
    "    max_k_indx = np.argpartition(l, k)[-k:]\n",
    "    return max_k_indx\n",
    "\n",
    "for index in top_k_indices([5, 0, 0, 4, 10, 1], k=2):\n",
    "    print(index, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|[]()|Median scores|\n",
       "|---|---|\n",
       "|**Original**|0.7902|\n",
       "|**PCA**|0.07671|\n",
       "|**LR**|0.3663|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features, labels = read_and_normalize('LLVMAll.csv')\n",
    "\n",
    "scores = {name: [] for name in ('original', 'pca', 'lr')}\n",
    "for seed in np.arange(1, 30+1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state=seed)\n",
    "    \n",
    "    # Your solution\n",
    "    lin_reg = LinearRegression().fit(X_train, y_train)\n",
    "    top_two_features_indices = top_k_indices(lin_reg.coef_, k=2)\n",
    "    \n",
    "    pca = PCA(n_components=2).fit(X_train)\n",
    "    \n",
    "    original_score = LinearRegression().fit(X_train, y_train).score(X_test, y_test)\n",
    "    scores[\"original\"].append(original_score)\n",
    "    \n",
    "    X_train_lr, X_test_lr = X_train[:, top_two_features_indices], X_test[:, top_two_features_indices]\n",
    "    lr_score = LinearRegression().fit(X_train_lr, y_train).score(X_test_lr, y_test)\n",
    "    scores[\"lr\"].append(lr_score)\n",
    "    \n",
    "    X_train_pca, X_test_pca = pca.transform(X_train), pca.transform(X_test)\n",
    "    pca_score = LinearRegression().fit(X_train_pca, y_train).score(X_test_pca, y_test)\n",
    "    scores[\"pca\"].append(pca_score)\n",
    "    \n",
    "median_original, median_pca, median_lr = np.median(scores[\"original\"]), np.median(scores[\"pca\"]), np.median(scores[\"lr\"])\n",
    "\n",
    "display(Markdown(\\\n",
    "f'|[]()|Median scores|\\n{\"|---\"*2}|\\n'\n",
    "f'|**Original**|{median_original:.4}|\\n'\n",
    "f'|**PCA**|{median_pca:.4}|\\n'\n",
    "f'|**LR**|{median_lr:.4}|\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain your observation with respect to relative performance of PCA and linear regression for dimension reduction and why this has happened in a few sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit this cell to answer TODOOOOOOOOOOOOOOOOOOO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Local vs. Global Prediction (30 points)\n",
    "\n",
    "In many situations, your training dataset is very large to include as many observations as possible, which is typically a good thing. For instance, a complex prediction model for image captioning works best if the learning dataset is massive and rich. However, if you are using a simple model (like a linear regression) for any reason (e.g., speed, interpretability, etc.) one caveat of very large datasets is that they might actually become very far off from your test set.\n",
    "For instance, assume you have to predict online sales for a particular book. But your training set is the entire Amazon’s historical sales records. Obviously a simple linear model will not work well, trying to fit a line that predicts sales of everything, from books, to grocery, to toys, etc.\n",
    "\n",
    "One simple solution could be training your model only on a portion of the training set that is more similar to the data you're interested in. For instance, in the above example, only train on the book records. \n",
    "\n",
    "In this part, we want to evaluate this idea automatically, by first clustering the training dataset and then fitting the model only on the closets cluster to the test data.\n",
    "\n",
    "\n",
    "- Read data from the csv dataset `filename='CPU_Performance.csv'`. The columns in the dataset are:\n",
    "    \n",
    "    - MYCT: machine cycle time in nanoseconds (integer) \n",
    "    - MMIN: minimum main memory in kilobytes (integer) \n",
    "    - MMAX: maximum main memory in kilobytes (integer) \n",
    "    - CACH: cache memory in kilobytes (integer) \n",
    "    - CHMIN: minimum channels in units (integer) \n",
    "    - CHMAX: maximum channels in units (integer) \n",
    "    - PRP: published relative performance (integer) \n",
    "    - ERP: estimated relative performance from the original article (integer)\n",
    "    \n",
    "    Use the last column (ERP) as the target and the others as features.\n",
    "\n",
    "- Take 80% of data as train and 20% as test using train_test_split with `random_state=0`\n",
    "- Build a linear SVM model and report its mean squared error on the test data\n",
    "- Cluster the training set to multiple clusters using Kmeans (K from 2 to 5 inclusive and `random_state=0`)\n",
    "- Find the most similar cluster to the test set. To do this find the cluster that most of the the test data fall into, if they are predicted using the cluster model.\n",
    "- Build a local linear svm model where you use only the closets cluster as your train dataset (all other setups unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global error=4060.1\n",
      "k=2 error=107.15\n",
      "k=3 error=41.933\n",
      "k=4 error=231.49\n",
      "k=5 error=149.52\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import mode\n",
    "import warnings\n",
    "# Ignore increasing max iterations warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Your solution\n",
    "# Split the data into traingin and testing sets as described above\n",
    "data = pd.read_csv(\"CPU_Performance.csv\")\n",
    "features = data.drop(columns='ERP')\n",
    "labels = data['ERP']\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state=0, test_size=0.2)\n",
    "\n",
    "# Train a LinearSVR model on data and report its score on test set\n",
    "model = LinearSVR(random_state=0).fit(X_train, y_train)\n",
    "y_prediction = model.predict(X_test)\n",
    "score = mean_squared_error(y_test, y_prediction)\n",
    "print(f'global error={score:.5}')\n",
    "\n",
    "for k in range(2, 5+1):\n",
    "    # Make a kMeans clustrer\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(X_train)\n",
    "    \n",
    "    # Find the cluster which will contains the majority of test points\n",
    "    y_prediction = kmeans.predict(X_test)\n",
    "    closest_cluster_id = mode(y_prediction, axis=None)[0][0]\n",
    "    \n",
    "    # Create a subset of training and test sets containing only the data that fall into the cluster above\n",
    "    X_train[\"ClusterId\"] = kmeans.predict(X_train)\n",
    "    Train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    Train_data = Train_data[Train_data[\"ClusterId\"] == closest_cluster_id]\n",
    "    \n",
    "    X_test[\"ClusterId\"] = y_prediction\n",
    "    Test_data = pd.concat([X_test, y_test], axis=1)\n",
    "    Test_data = Test_data[Test_data[\"ClusterId\"] == closest_cluster_id]\n",
    "    \n",
    "    local_X_train, local_y_train = Train_data.drop(columns=\"ERP\"), Train_data[\"ERP\"]\n",
    "    local_X_test, local_y_test = Test_data.drop(columns=\"ERP\"), Test_data[\"ERP\"]\n",
    "    \n",
    "    # Train a LinearSVR model on the local data and report its mean squared error on test set\n",
    "    model = LinearSVR(random_state=0).fit(local_X_train, local_y_train)\n",
    "    local_y_pred = model.predict(local_X_test)\n",
    "    score = mean_squared_error(local_y_test, local_y_pred)\n",
    "    print(f'k={k} error={score:.5}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the error go down or up? Do you think it is a good practice or it's getting better (if it did) because some information is leaking from the test set?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
