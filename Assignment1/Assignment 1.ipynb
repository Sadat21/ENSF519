{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSF 519.01 Applied Data Science \n",
    "**Assignment 1** - 100 marks\n",
    "\n",
    "**Due:** October 4th, 04.00 pm.\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE: each task must be implemented as asked, even if there are other easier or better solutions.**\n",
    "\n",
    "**How to deliver:**\n",
    "Edit this file and write your solutions in sections specified with `# Your solution`. Test your code and when you were done, submit this notebook as an `.ipynb` file to D2L dropbox. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - The Zipf mystery (50 points)\n",
    "\n",
    "In this problem, we'd like to read the text from a book and perform some simple statistical analysis on the word counts. We have provided you with the actual text from [Lost On The Moon or, In Quest of the Field of Diamonds](https://www.goodreads.com/book/show/8636132-lost-on-the-moon-or-in-quest-of-the-field-of-diamonds) book in a file named 'the book.txt'. The file is cleaned up and only contains alphanumeric characters, i.e. no punctuation, quotation marks, etc.\n",
    "\n",
    "Read the file and break it down to its words. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'latter', 'picked', 'it', 'up', 'gazed', 'at', 'it', 'first', 'from']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_and_tokenize(file_name):\n",
    "    # Your solution\n",
    "    file = open(file_name, \"r\")\n",
    "    bookString = file.read()\n",
    "    return bookString.split()\n",
    "        \n",
    "\n",
    "words = read_and_tokenize('the book.txt')\n",
    "words[1101:1111] # Expected: ['the', 'latter', 'picked', 'it', 'up', 'gazed', 'at', 'it', 'first', 'from']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a sorted list of unique words in the book. Store the list in a variable called `V`. Also complete the `get_word_index` function below that gets a word and finds its index within `V`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution goes here\n",
    "uniqueWords = set(words)\n",
    "V = sorted(uniqueWords)\n",
    "def get_word_index(word):\n",
    "    for index, val in enumerate(V):\n",
    "        if val == word:\n",
    "            return index\n",
    "\n",
    "get_word_index('about')  # Expected: 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using no loops, and by only using `map` and `filter` built-in python functions traverse through the `V` (vocabulary) list above to find:\n",
    "\n",
    "* `long_words`: The list of words that have 10 letters or more \n",
    "* `no_vowels`: A list of all words but with vowels (aoeiu) removed. You can nest `map` and `filter` calls to iterate through the characters of the words.\n",
    "\n",
    "(5+5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "long_words = list(filter(lambda word: len(word) >= 10, V))\n",
    "no_vowels = list(map( lambda word: ''.join(filter(lambda c : c not in 'aeiou', word)), V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a numpy array of size `|V|` that only contains 0s. Store it in a variable named `frequencies`. Use this array to count the number of times each word has appeared in the book. For example `frequencies[9]` should store how many times the word located in the index 9 of `V` (the sorted list) --which is the word \"about\"-- has been appreaed in the book (165 times). (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.,  1., ..., 11.,  1.,  1.]), 165.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Your solution\n",
    "frequencies = np.zeros(len(V))\n",
    "\n",
    "for word in words:\n",
    "    index = get_word_index(word)\n",
    "    frequencies[index] = frequencies[index] + 1\n",
    "\n",
    "frequencies, frequencies[9] # Expected: array([ 1.,  1.,  1., ..., 11.,  1.,  1.]), 165.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the word that appeared most frequently in the book. Find the word itself as well as the number of times it was repeated in the book. Use numpy functions, i.e. do not iterate over the `frequencies` array manually using a `for` loop. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\" is the most common word which has appeared 3237 times in the book.\n"
     ]
    }
   ],
   "source": [
    "# Your solution \n",
    "max_frequency = int(np.amax(frequencies))\n",
    "rv = np.where(frequencies == max_frequency)\n",
    "most_common_word = V[rv[0][0]]\n",
    "\n",
    "print(f'\"{most_common_word}\" is the most common word which has appeared {max_frequency} times in the book.')\n",
    "# Expected: \"the\" is the most common word which has appeared 3237 times in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize all frequency values by dividing them by the maximum frequency value (using vectorized operators). After this the most common word in the book should get a normalized frequency of `1` and all other words get some value \n",
    "between `1/MAX` and `1`. (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00030893, 0.00030893, 0.00030893, ..., 0.00339821, 0.00030893,\n",
       "       0.00030893])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution\n",
    "normalized_frequencies = frequencies / max_frequency\n",
    "normalized_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check if the normalized frequencies have any corelation to their ranks. If such correlation exists, the Zipf's law states that it is linear in a log-log space. Take the logarithm of normalized frequencies (as y values) and create a numpy array of the same size containing the rank of each word (as x values). For example if the frequencies array is `[0.1, 1, 0.01, 0.0001]` the x and y values will be `X = [2, 1, 3, 4] Y=[-1, 0, -2, -4]`. \n",
    "\n",
    "You might want to sort the normalized frequencies first to make the task easier. (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4319 3009 3010 ...    2    1    0] [-3.5101427  -3.5101427  -3.5101427  ... -0.38825471 -0.37980893\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Your solution \n",
    "sorted_normalized = np.sort(normalized_frequencies)\n",
    "\n",
    "temp = sorted_normalized.argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.flip(np.arange(len(sorted_normalized)))\n",
    "    \n",
    "x = ranks\n",
    "y = np.log10(sorted_normalized)\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the [pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) on this data. The result is expected to be close to -1. Define appropriate functions for the the statistical calculations as neccessary. Additionally, you can use `pearsonr` function from `scipy` package to check if the calculated value is definitely correct. Though if you get a value close enough to -1 you can almost be sure that your implementation is correct and this step won't be necessary. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8631989095267899"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution goes here\n",
    "def pcc(x, y):\n",
    "    mean_x = np.mean(x)\n",
    "    mean_y = np.mean(y)\n",
    "    \n",
    "    temp = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        temp[i] = (x[i] - mean_x) * (y[i] - mean_y)\n",
    "    \n",
    "    cov = np.average(temp)\n",
    "    rv = cov / (np.std(x) * np.std(y))\n",
    "    return rv\n",
    "\n",
    "pcc(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Log processing (50 points)\n",
    "\n",
    "In this part of the assignment we are going to use regular expressions to mine data out of some webserver log files. Although these problems can be solved without use of RegExes, but for this assignment you need to use them.\n",
    "\n",
    "A sample web server log file is provided along with this problem. In each line of the file one event is recorded. For simplicity all of the events in this file have the same format and are of the same type. Each event contains an ip address, date and time of the event, http method (`GET` or `POST`), a url, HTTP version, HTTP response code (usually 200), the response size in bytes, and the device's user agent which contains information about the device such as the brand and the operating system.\n",
    "\n",
    "Since these logs have such a well defined format regular expressions are the prefect tool for breaking them down into parts and perform different analysis on them.\n",
    "\n",
    "**Please make sure that when you are asked to write a function that _return_s something, you are _return_ing that value, not just _print_ing it**\n",
    "\n",
    "We start off with a random log line and write python functions that use regular expressions to break it off to pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.106.145.204 - - [04/Sep/2019:13:51:39 +0430] \"POST /v1/crash-report/incident/report/ HTTP/1.1\" 200 65 \"-\" \"Dalvik/1.6.0 (Linux; U; Android 4.2.2; GT-S7272 Build/JDQ39)\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "l = '5.106.145.204 - - [04/Sep/2019:13:51:39 +0430] \"POST /v1/crash-report/incident/report/ HTTP/1.1\" 200 65 \"-\" \"Dalvik/1.6.0 (Linux; U; Android 4.2.2; GT-S7272 Build/JDQ39)\"'\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that extracts the ip address part of the log line using regular expressions. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.106.145.204'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ip_address(l):\n",
    "    # Your solution here\n",
    "    rv = re.search('^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', l).group()\n",
    "    return rv\n",
    "\n",
    "\n",
    "get_ip_address(l)  # Expected: '5.106.145.204'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that extracts the HTTP method, url, response code, and response size and returns a tuple. Use regular expressions. The http method is either `POST` or `GET` and the response code is always a 3 digit integer. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('POST', '/v1/crash-report/incident/report/', 200, 65)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_http_info(l):\n",
    "    # Your solution here\n",
    "    method = re.findall('(GET|POST)\\s', l)[0]\n",
    "    url = re.findall(' (/\\S*/*)', l)[0]\n",
    "    response_code, response_size = re.findall('\\\" (\\d{3}) (\\d+) \\\"', l)[0]\n",
    "    \n",
    "    return (method, url, int(response_code), int(response_size))\n",
    "\n",
    "get_http_info(l)  # Expected: ('POST', '/v1/crash-report/incident/report/', 200, 65)\n",
    "# Please note that the last two numbers are converted to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use regular expressions to break the date and time section apart and create a python datetime object based on that. Mind the time zone. convert the datetimes to MDT. Using `strptime` is a better solution in general, but for this assignment please stick to writing RegExes so you become more comfortable in writing and debugging them. (20 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 9, 4, 3, 21, 39, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=64800)))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "from calendar import month_abbr\n",
    "\n",
    "MDT = timezone(timedelta(minutes=-6*60 + 0))\n",
    "\n",
    "def get_datetime(l):\n",
    "    # Your solution here    \n",
    "    day, month, year, hour, minute, second, offset_sign, offset_hour, offset_minute = re.findall('\\[(\\d{1,2})/(\\w{3,4})/(\\d{4}):(\\d{2}):(\\d{2}):(\\d{2}) ([+-])(\\d{2})(\\d{2})]', l)[0]\n",
    "    \n",
    "    var = list(month_abbr)\n",
    "    month = var.index(month)\n",
    "    \n",
    "    zone = timezone(timedelta(minutes=int(offset_sign + offset_hour)*60 + int(offset_sign + offset_minute)))\n",
    "    \n",
    "    rv = datetime(int(year), month, int(day), int(hour), int(minute), int(second), 0, zone)                        \n",
    "    return rv.astimezone(MDT)\n",
    "\n",
    "get_datetime(l)  # Expected: datetime.datetime(2019, 9, 4, 3, 21, 39, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=64800)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the log file line by line and use the `get_datetime` and `get_http_info` functions above to calculate the used bandwidth of the server (the sum of all the response sizes) per hour. Use a `dict` or a `defaultdict`. (15 points)\n",
    "\n",
    "For example if there are 4 logs like:\n",
    "\n",
    "    Sep 4 14:20 .... 65bytes\n",
    "    Sep 4 14:35 .... 80bytes\n",
    "    Sep 4 15:01 .... 44bytes\n",
    "    Sep 5 18:20 .... 40bytes\n",
    "\n",
    "The result will be like:\n",
    "\n",
    "    Sep 4 14:00  145\n",
    "    Sep 4 15:00  44\n",
    "    Sep 5 18:00  40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-20 07:00 49130 bytes\n",
      "2019-07-20 08:00 40469 bytes\n",
      "2019-07-20 09:00 43556 bytes\n",
      "2019-07-20 10:00 82526 bytes\n",
      "2019-07-20 11:00 56328 bytes\n",
      "2019-07-20 12:00 98862 bytes\n",
      "2019-07-20 13:00 119679 bytes\n",
      "2019-07-20 14:00 57126 bytes\n",
      "2019-07-20 15:00 135680 bytes\n",
      "2019-07-20 16:00 48710 bytes\n",
      "2019-07-20 17:00 45631 bytes\n",
      "2019-07-20 18:00 9805 bytes\n",
      "2019-07-20 19:00 3569 bytes\n",
      "2019-07-20 20:00 36087 bytes\n",
      "2019-07-20 21:00 55406 bytes\n",
      "2019-07-20 22:00 68764 bytes\n",
      "2019-07-20 23:00 30101 bytes\n",
      "2019-07-21 00:00 83251 bytes\n",
      "2019-07-21 01:00 77896 bytes\n",
      "2019-07-21 02:00 65166 bytes\n",
      "2019-07-21 03:00 66326 bytes\n",
      "2019-07-21 04:00 86495 bytes\n",
      "2019-07-21 05:00 76888 bytes\n",
      "2019-07-21 06:00 65378 bytes\n",
      "2019-07-21 07:00 116337 bytes\n",
      "2019-07-21 08:00 55348 bytes\n",
      "2019-07-21 09:00 40975 bytes\n",
      "2019-07-21 10:00 51204 bytes\n",
      "2019-07-21 11:00 56527 bytes\n",
      "2019-07-21 12:00 50933 bytes\n",
      "2019-07-21 13:00 29773 bytes\n",
      "2019-07-21 14:00 80279 bytes\n",
      "2019-07-21 15:00 68270 bytes\n",
      "2019-07-21 16:00 12714 bytes\n",
      "2019-07-21 17:00 6414 bytes\n",
      "2019-07-21 18:00 9186 bytes\n",
      "2019-07-21 19:00 4356 bytes\n",
      "2019-07-21 20:00 17380 bytes\n",
      "2019-07-21 21:00 20251 bytes\n",
      "2019-07-21 22:00 25015 bytes\n",
      "2019-07-21 23:00 25326 bytes\n",
      "2019-07-22 00:00 21912 bytes\n",
      "2019-07-22 01:00 27967 bytes\n",
      "2019-07-22 02:00 56146 bytes\n",
      "2019-07-22 03:00 100773 bytes\n",
      "2019-07-22 04:00 87930 bytes\n",
      "2019-07-22 05:00 77608 bytes\n",
      "2019-07-22 06:00 84938 bytes\n",
      "2019-07-22 07:00 114074 bytes\n",
      "2019-07-22 08:00 139403 bytes\n",
      "2019-07-22 09:00 91115 bytes\n",
      "2019-07-22 10:00 85637 bytes\n",
      "2019-07-22 11:00 113985 bytes\n",
      "2019-07-22 12:00 160342 bytes\n",
      "2019-07-22 13:00 147641 bytes\n",
      "2019-07-22 14:00 120636 bytes\n",
      "2019-07-22 15:00 81082 bytes\n",
      "2019-07-22 16:00 66001 bytes\n",
      "2019-07-22 17:00 15379 bytes\n",
      "2019-07-22 18:00 17998 bytes\n",
      "2019-07-22 19:00 10826 bytes\n",
      "2019-07-22 20:00 24312 bytes\n",
      "2019-07-22 21:00 33147 bytes\n",
      "2019-07-22 22:00 24325 bytes\n",
      "2019-07-22 23:00 27712 bytes\n",
      "2019-07-23 00:00 63858 bytes\n",
      "2019-07-23 01:00 44863 bytes\n",
      "2019-07-23 02:00 28687 bytes\n",
      "2019-07-23 03:00 82434 bytes\n",
      "2019-07-23 04:00 91401 bytes\n",
      "2019-07-23 05:00 84796 bytes\n",
      "2019-07-23 06:00 72745 bytes\n",
      "2019-07-23 07:00 35005 bytes\n",
      "2019-07-23 08:00 72550 bytes\n",
      "2019-07-23 09:00 90913 bytes\n",
      "2019-07-23 10:00 86130 bytes\n",
      "2019-07-23 11:00 73650 bytes\n",
      "2019-07-23 12:00 58672 bytes\n",
      "2019-07-23 13:00 95681 bytes\n",
      "2019-07-23 14:00 72453 bytes\n",
      "2019-07-23 15:00 47086 bytes\n",
      "2019-07-23 16:00 7968 bytes\n",
      "2019-07-23 17:00 4677 bytes\n",
      "2019-07-23 18:00 10726 bytes\n",
      "2019-07-23 19:00 9506 bytes\n",
      "2019-07-23 20:00 6394 bytes\n",
      "2019-07-23 21:00 25995 bytes\n",
      "2019-07-23 22:00 64144 bytes\n",
      "2019-07-23 23:00 24542 bytes\n",
      "2019-07-24 00:00 29869 bytes\n",
      "2019-07-24 01:00 39192 bytes\n",
      "2019-07-24 02:00 44674 bytes\n",
      "2019-07-24 03:00 80986 bytes\n",
      "2019-07-24 04:00 50236 bytes\n",
      "2019-07-24 05:00 100834 bytes\n",
      "2019-07-24 06:00 57701 bytes\n",
      "2019-07-24 07:00 58621 bytes\n",
      "2019-07-24 08:00 75741 bytes\n",
      "2019-07-24 09:00 20267 bytes\n",
      "2019-07-24 10:00 48225 bytes\n",
      "2019-07-24 11:00 45402 bytes\n",
      "2019-07-24 12:00 52613 bytes\n",
      "2019-07-24 13:00 113918 bytes\n",
      "2019-07-24 14:00 58613 bytes\n",
      "2019-07-24 15:00 75026 bytes\n",
      "2019-07-24 16:00 107463 bytes\n",
      "2019-07-24 17:00 6480 bytes\n",
      "2019-07-24 18:00 3702 bytes\n",
      "2019-07-24 19:00 9946 bytes\n",
      "2019-07-24 20:00 11825 bytes\n",
      "2019-07-24 21:00 13964 bytes\n",
      "2019-07-24 22:00 15936 bytes\n",
      "2019-07-24 23:00 25976 bytes\n",
      "2019-07-25 00:00 38511 bytes\n",
      "2019-07-25 01:00 44458 bytes\n",
      "2019-07-25 02:00 50009 bytes\n",
      "2019-07-25 03:00 74315 bytes\n",
      "2019-07-25 04:00 48235 bytes\n",
      "2019-07-25 05:00 27275 bytes\n",
      "2019-07-25 06:00 38408 bytes\n",
      "2019-07-25 07:00 35241 bytes\n",
      "2019-07-25 08:00 42373 bytes\n",
      "2019-07-25 09:00 62963 bytes\n",
      "2019-07-25 10:00 28674 bytes\n",
      "2019-07-25 11:00 77992 bytes\n",
      "2019-07-25 12:00 45697 bytes\n",
      "2019-07-25 13:00 48120 bytes\n",
      "2019-07-25 14:00 16894 bytes\n",
      "2019-07-25 15:00 11946 bytes\n",
      "2019-07-25 16:00 9182 bytes\n",
      "2019-07-25 17:00 4174 bytes\n",
      "2019-07-25 18:00 5998 bytes\n",
      "2019-07-25 19:00 22558 bytes\n",
      "2019-07-25 20:00 4467 bytes\n",
      "2019-07-25 21:00 13011 bytes\n",
      "2019-07-25 22:00 15906 bytes\n",
      "2019-07-25 23:00 11325 bytes\n",
      "2019-07-26 00:00 47222 bytes\n",
      "2019-07-26 01:00 47935 bytes\n",
      "2019-07-26 02:00 40544 bytes\n",
      "2019-07-26 03:00 72866 bytes\n",
      "2019-07-26 04:00 11055 bytes\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "\n",
    "# No specific format for the output is expected\n",
    "# However the data will be something like:\n",
    "#  2019, 7, 20 07:00    49130 bytes\n",
    "#  2019, 7, 20 08:00    40469 bytes\n",
    "#  2019, 7, 20 09:00    43556 bytes\n",
    "#  2019, 7, 20 10:00    82526 bytes .... \n",
    "\n",
    "file = open('log.txt', 'r')\n",
    "lines = file.readlines()\n",
    "\n",
    "ddd = {}\n",
    "\n",
    "for line in lines:\n",
    "    bandwidth = get_http_info(line)[3]\n",
    "    datetime_val = get_datetime(line)\n",
    "    cleaned_date = re.findall('\\d{4}-\\d{2}-\\d{2} \\d{2}', str(datetime_val))[0] + ':00'\n",
    "    \n",
    "    if cleaned_date in ddd.keys():\n",
    "        ddd[cleaned_date] = ddd[cleaned_date] + bandwidth\n",
    "        \n",
    "    else:\n",
    "        ddd[cleaned_date] = bandwidth\n",
    "        \n",
    "for key, value in ddd.items():\n",
    "    print(key, str(value) + ' bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
